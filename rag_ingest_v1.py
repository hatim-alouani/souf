import os
import logging
import shutil
import re
from typing import List, Dict, Tuple
from dataclasses import dataclass

import fitz  # PyMuPDF
import torch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder

# -------------------------------
# Logging Configuration
# -------------------------------
if not os.path.exists('logs'):
    os.makedirs('logs')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/rag_agent.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# -------------------------------
# Paths
# -------------------------------
DATA_DIR = "data/raw"
DB_DIR = "vectorstore"

# -------------------------------
# Chemical Formula Normalizer (Enhanced)
# -------------------------------
class ChemicalNormalizer:
    """
    Normalise les formules chimiques et pr√©serve les notations techniques.
    """
    
    # Dictionnaire de synonymes chimiques pour OCP
    CHEMICAL_SYNONYMS = {
        "acide phosphorique": ["H3PO4", "phosphoric acid"],
        "acide sulfurique": ["H2SO4", "sulfuric acid"],
        "phosphate": ["PO4", "phosphate rock"],
        "sulfate": ["SO4"],
        "TSP": ["triple superphosphate", "Ca(H2PO4)2"],
        "MAP": ["monoammonium phosphate", "NH4H2PO4"],
        "DAP": ["diammonium phosphate", "(NH4)2HPO4"],
    }
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """
        Am√©liore le texte extrait des PDFs techniques.
        """
        # Convertir subscripts Unicode -> nombres normaux
        subscript_map = str.maketrans('‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ', '0123456789')
        text = text.translate(subscript_map)
        
        # Convertir superscripts
        superscript_map = str.maketrans('‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ‚Å∫‚Åª', '0123456789+-')
        text = text.translate(superscript_map)
        
        # Normaliser les espaces multiples
        text = re.sub(r'\s+', ' ', text)
        
        # Pr√©server les pourcentages
        text = re.sub(r'(\d+)\s*%', r'\1%', text)
        
        # Pr√©server les temp√©ratures
        text = re.sub(r'(\d+)\s*¬∞C', r'\1¬∞C', text)
        
        # Normaliser les ranges de valeurs
        text = re.sub(r'(\d+)\s*-\s*(\d+)', r'\1-\2', text)
        
        return text.strip()
    
    @staticmethod
    def extract_entities(text: str) -> Dict[str, List[str]]:
        """Extrait les entit√©s chimiques importantes."""
        entities = {
            "formulas": [],
            "concentrations": [],
            "temperatures": [],
            "equipment": []
        }
        
        # Formules chimiques (ex: H2SO4, P2O5)
        formulas = re.findall(r'\b[A-Z][a-z]?\d*(?:[A-Z][a-z]?\d*)*\b', text)
        entities["formulas"] = list(set(f for f in formulas if len(f) > 1))
        
        # Concentrations (ex: 28-30%, 54% P2O5)
        concentrations = re.findall(r'\d+(?:\.\d+)?(?:-\d+(?:\.\d+)?)?%?\s*(?:P2O5|H2SO4|H3PO4)?', text)
        entities["concentrations"] = list(set(concentrations))
        
        # Temp√©ratures
        temperatures = re.findall(r'\d+(?:\.\d+)?¬∞C', text)
        entities["temperatures"] = list(set(temperatures))
        
        # √âquipements (filtres, r√©acteurs, etc.)
        equipment_patterns = [
            r'filtre\w*', r'r√©acteur\w*', r'cristalliseur\w*',
            r'√©vaporateur\w*', r'broyeur\w*', r's√©cheur\w*'
        ]
        for pattern in equipment_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities["equipment"].extend(matches)
        
        entities["equipment"] = list(set(entities["equipment"]))
        
        return entities
    
    @staticmethod
    def is_section_header(text: str) -> bool:
        """D√©tecte si le texte est un titre de section."""
        patterns = [
            r'^\d+\.\s+[A-Z]',
            r'^[A-Z\s]{5,}$',
            r'^Section\s+\d+',
            r'^CHAPITRE\s+\d+',
            r'^Annexe\s+[A-Z\d]',
        ]
        return any(re.match(pattern, text.strip()) for pattern in patterns)

# -------------------------------
# Enhanced PDF Extraction with Metadata
# -------------------------------
def extract_text_from_pdf(pdf_path: str) -> List[Dict]:
    """
    Extraction am√©lior√©e avec pr√©servation de structure et m√©tadonn√©es riches.
    """
    doc_content = []
    normalizer = ChemicalNormalizer()
    
    try:
        doc = fitz.open(pdf_path)
        current_section = "Introduction"
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # Extraire avec structure (blocks)
            blocks = page.get_text("dict")["blocks"]
            page_text_parts = []
            
            for block in blocks:
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            text = span["text"].strip()
                            if text:
                                if normalizer.is_section_header(text):
                                    current_section = text
                                page_text_parts.append(text)
            
            full_text = " ".join(page_text_parts)
            full_text = normalizer.normalize_text(full_text)
            
            if full_text.strip():
                # Extraire les entit√©s
                entities = normalizer.extract_entities(full_text)
                
                doc_content.append({
                    "page_content": full_text,
                    "metadata": {
                        "source": os.path.basename(pdf_path),
                        "page": page_num + 1,
                        "section": current_section,
                        "type_doc": "technique",
                        "chemical_formulas": ", ".join(entities["formulas"][:5]),
                        "has_concentrations": len(entities["concentrations"]) > 0,
                        "has_temperatures": len(entities["temperatures"]) > 0,
                        "equipment_mentioned": ", ".join(entities["equipment"][:3]),
                    }
                })
        
        doc.close()
        logger.info(f"‚úÖ Extracted {len(doc_content)} pages from {os.path.basename(pdf_path)}")
        
    except Exception as e:
        logger.error(f"‚ùå Error reading PDF {pdf_path}: {e}")
    
    return doc_content

# -------------------------------
# E5 Embeddings with Prefix
# -------------------------------
class E5EmbeddingsWithPrefix(HuggingFaceEmbeddings):
    """Wrapper pour ajouter les pr√©fixes requis par E5."""
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        prefixed_texts = [f"passage: {text}" for text in texts]
        return super().embed_documents(prefixed_texts)
    
    def embed_query(self, text: str) -> List[float]:
        prefixed_text = f"query: {text}"
        return super().embed_query(prefixed_text)

# -------------------------------
# Technical Text Splitter (Enhanced)
# -------------------------------
class TechnicalTextSplitter(RecursiveCharacterTextSplitter):
    """Splitter adapt√© aux documents techniques chimiques."""
    
    def __init__(self, chunk_size=800, chunk_overlap=150):
        separators = [
            "\n\n\n",
            "\n\n",
            "\n",
            ". ",
            ".",
            ";",
            ",",
        ]
        
        super().__init__(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators,
            length_function=len,
            is_separator_regex=False
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """Split avec enrichissement m√©tadata."""
        chunks = super().split_documents(documents)
        
        normalizer = ChemicalNormalizer()
        for i, chunk in enumerate(chunks):
            # Ajouter un ID unique
            chunk.metadata["chunk_id"] = f"{chunk.metadata.get('source', 'unknown')}_{i}"
            
            # Extraire les entit√©s du chunk
            entities = normalizer.extract_entities(chunk.page_content)
            chunk.metadata.update({
                "has_chemical_formula": len(entities["formulas"]) > 0,
                "has_percentage": "%" in chunk.page_content,
                "has_temperature": len(entities["temperatures"]) > 0,
                "has_equation": "‚Üí" in chunk.page_content or "=" in chunk.page_content,
                "word_count": len(chunk.page_content.split()),
                "char_count": len(chunk.page_content)
            })
        
        return chunks

# -------------------------------
# RRF (Reciprocal Rank Fusion)
# -------------------------------
@dataclass
class RRFResult:
    """R√©sultat RRF avec score et document."""
    document: Document
    rrf_score: float
    original_ranks: Dict[str, int]

def reciprocal_rank_fusion(
    search_results_list: List[List[Document]],
    k: int = 60
) -> List[RRFResult]:
    """
    Impl√©mente RRF pour fusionner les r√©sultats de plusieurs recherches.
    
    Args:
        search_results_list: Liste de listes de documents (une par m√©thode de recherche)
        k: Constante RRF (typiquement 60)
    
    Returns:
        Liste de RRFResult tri√©e par score d√©croissant
    """
    # Dictionnaire: chunk_id -> (Document, {method: rank})
    doc_scores = {}
    
    for method_idx, results in enumerate(search_results_list):
        for rank, doc in enumerate(results, start=1):
            chunk_id = doc.metadata.get("chunk_id", f"doc_{id(doc)}")
            
            if chunk_id not in doc_scores:
                doc_scores[chunk_id] = {
                    "document": doc,
                    "ranks": {},
                    "rrf_score": 0.0
                }
            
            # RRF formula: 1 / (k + rank)
            doc_scores[chunk_id]["ranks"][f"method_{method_idx}"] = rank
            doc_scores[chunk_id]["rrf_score"] += 1.0 / (k + rank)
    
    # Convertir en RRFResult et trier
    rrf_results = [
        RRFResult(
            document=data["document"],
            rrf_score=data["rrf_score"],
            original_ranks=data["ranks"]
        )
        for data in doc_scores.values()
    ]
    
    rrf_results.sort(key=lambda x: x.rrf_score, reverse=True)
    
    return rrf_results

# -------------------------------
# Cross-Encoder Re-ranking
# -------------------------------
class CrossEncoderReranker:
    """
    Re-ranking avec Cross-Encoder pour am√©liorer la pertinence.
    """
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """
        Initialise le cross-encoder.
        
        Mod√®les recommand√©s:
        - cross-encoder/ms-marco-MiniLM-L-6-v2 (rapide, bon √©quilibre)
        - cross-encoder/ms-marco-MiniLM-L-12-v2 (plus pr√©cis)
        - amberoad/bert-multilingual-passage-reranking-msmarco (multilingue)
        """
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"üîÑ Loading Cross-Encoder: {model_name} (device: {device})")
        
        self.model = CrossEncoder(model_name, device=device)
        self.model_name = model_name
    
    def rerank(
        self,
        query: str,
        documents: List[Document],
        top_k: int = 10
    ) -> List[Tuple[Document, float]]:
        """
        Re-rank les documents avec le cross-encoder.
        
        Args:
            query: Question de l'utilisateur
            documents: Documents √† re-ranker
            top_k: Nombre de documents √† retourner
        
        Returns:
            Liste de (Document, score) tri√©e par score d√©croissant
        """
        if not documents:
            return []
        
        # Pr√©parer les paires (query, document)
        pairs = [[query, doc.page_content] for doc in documents]
        
        # Calculer les scores
        scores = self.model.predict(pairs, show_progress_bar=False)
        
        # Combiner documents et scores
        doc_scores = list(zip(documents, scores))
        
        # Trier par score d√©croissant
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return doc_scores[:top_k]

# -------------------------------
# Hybrid Retriever with RRF and Re-ranking
# -------------------------------
class HybridRetrieverWithReranking:
    """
    R√©cup√©rateur hybride combinant:
    1. Recherche s√©mantique (embeddings)
    2. Recherche par similarit√© avec diff√©rents param√®tres
    3. RRF pour fusion
    4. Cross-encoder pour re-ranking final
    """
    
    def __init__(
        self,
        vectorstore: Chroma,
        reranker: CrossEncoderReranker,
        k_semantic: int = 20,
        k_mmr: int = 20,
        k_final: int = 5
    ):
        self.vectorstore = vectorstore
        self.reranker = reranker
        self.k_semantic = k_semantic
        self.k_mmr = k_mmr
        self.k_final = k_final
    
    def retrieve(self, query: str) -> List[Tuple[Document, float]]:
        """
        R√©cup√®re les documents les plus pertinents.
        
        Pipeline:
        1. Recherche s√©mantique (similarity)
        2. Recherche MMR (Maximum Marginal Relevance)
        3. RRF pour fusionner
        4. Cross-encoder re-ranking
        """
        logger.info(f"üîç Hybrid retrieval for: '{query}'")
        
        # 1. Recherche s√©mantique classique
        semantic_results = self.vectorstore.similarity_search(
            query,
            k=self.k_semantic
        )
        logger.info(f"  üìä Semantic search: {len(semantic_results)} results")
        
        # 2. Recherche MMR (diversit√©)
        mmr_results = self.vectorstore.max_marginal_relevance_search(
            query,
            k=self.k_mmr,
            fetch_k=self.k_mmr * 2
        )
        logger.info(f"  üìä MMR search: {len(mmr_results)} results")
        
        # 3. RRF Fusion
        rrf_results = reciprocal_rank_fusion(
            [semantic_results, mmr_results],
            k=60
        )
        logger.info(f"  üîó RRF fusion: {len(rrf_results)} unique documents")
        
        # Prendre les top documents apr√®s RRF
        top_rrf_docs = [r.document for r in rrf_results[:self.k_semantic]]
        
        # 4. Cross-Encoder Re-ranking
        reranked_results = self.reranker.rerank(
            query,
            top_rrf_docs,
            top_k=self.k_final
        )
        logger.info(f"  ‚≠ê Re-ranked: {len(reranked_results)} final results")
        
        return reranked_results

# -------------------------------
# Document Ingestion (Enhanced)
# -------------------------------
def ingest_documents():
    logger.info("üöÄ Starting enhanced ingestion with RRF + Re-ranking support...")

    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        logger.warning(f"{DATA_DIR} created. Please add your PDFs here.")
        return

    pdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.pdf')]
    if not pdf_files:
        logger.warning(f"No PDFs found in {DATA_DIR}")
        return

    # ‚ø° Extract with enhanced metadata
    raw_documents = []
    for pdf_file in pdf_files:
        pdf_path = os.path.join(DATA_DIR, pdf_file)
        logger.info(f"üìÑ Processing {pdf_file}...")
        extracted = extract_text_from_pdf(pdf_path)
        for data in extracted:
            raw_documents.append(Document(
                page_content=data["page_content"],
                metadata=data["metadata"]
            ))

    logger.info(f"‚úÖ Extraction: {len(raw_documents)} pages")

    # ‚ø¢ Enhanced Chunking
    splitter = TechnicalTextSplitter(
        chunk_size=800,
        chunk_overlap=150
    )
    chunks = splitter.split_documents(raw_documents)
    logger.info(f"‚úÖ Chunking: {len(chunks)} chunks")
    
    if chunks:
        avg_size = sum(len(c.page_content) for c in chunks) / len(chunks)
        logger.info(f"   Average chunk: {avg_size:.0f} chars")

    # ‚ø£ Embeddings
    device = 'cuda' if torch.cuda.is_available() and os.environ.get("USE_GPU", "1") == "1" else 'cpu'
    logger.info(f"üß† Generating embeddings (Device: {device})...")

    embeddings = E5EmbeddingsWithPrefix(
        model_name="intfloat/multilingual-e5-large",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    # ‚ø§ Rebuild vectorstore
    if os.path.exists(DB_DIR):
        logger.info(f"‚ö†  Rebuilding vectorstore...")
        shutil.rmtree(DB_DIR)

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=DB_DIR
    )

    logger.info(f"‚úÖ Ingestion completed! {len(chunks)} chunks indexed")
    
    # ‚ø• Test Hybrid Retrieval with Re-ranking
    logger.info("\nüß™ Testing Hybrid Retrieval with Re-ranking...")
    
    reranker = CrossEncoderReranker()
    hybrid_retriever = HybridRetrieverWithReranking(
        vectorstore=vectorstore,
        reranker=reranker,
        k_semantic=20,
        k_mmr=20,
        k_final=5
    )
    
    test_queries = [
        "concentration P2O5 acide phosphorique apr√®s filtration",
        "temp√©rature de cristallisation",
        "√©quipements de filtration utilis√©s"
    ]
    
    for query in test_queries:
        logger.info(f"\nüìù Query: '{query}'")
        results = hybrid_retriever.retrieve(query)
        
        for i, (doc, score) in enumerate(results, 1):
            logger.info(f"   {i}. Score: {score:.4f} | Source: {doc.metadata.get('source')} (p.{doc.metadata.get('page')})")
            logger.info(f"      Preview: {doc.page_content[:100]}...")

# -------------------------------
# Entry Point
# -------------------------------
if __name__ == "__main__":
    ingest_documents()
